{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94823be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c94823be",
        "outputId": "10186512-7883-45c2-cdbf-0ea81a9f9757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "import torch, random, numpy as np\n",
        "import os, time\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcbd2d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afcbd2d8",
        "outputId": "d7d9ccbe-5f45-4acf-ec15-cd3afd3c0ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rmisra/news-headlines-dataset-for-sarcasm-detection?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.30M/3.30M [00:00<00:00, 5.33MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/nikhiljohnk/tweets-with-sarcasm-and-irony?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.27M/4.27M [00:00<00:00, 6.20MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/sherinclaudia/sarcastic-comments-on-reddit?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 106M/106M [00:03<00:00, 33.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/tmp/ipython-input-822199672.py:57: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(n=int(take.loc[x.name]), random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val (Reddit) size: 222664\n",
            "Test (Non-Reddit) size: 121582\n",
            "Test sources:\n",
            " source\n",
            "twitter     66254\n",
            "headline    55328\n",
            "Name: count, dtype: int64\n",
            "Train/Val label balance:\n",
            " is_sarcastic\n",
            "0    111332\n",
            "1    111332\n",
            "Name: count, dtype: int64\n",
            "Train size: 200397 Val size: 22267\n"
          ]
        }
      ],
      "source": [
        "# Load data from kaggle\n",
        "headlinespath = kagglehub.dataset_download(\"rmisra/news-headlines-dataset-for-sarcasm-detection\")\n",
        "twitterpath   = kagglehub.dataset_download(\"nikhiljohnk/tweets-with-sarcasm-and-irony\")\n",
        "redditpath    = kagglehub.dataset_download(\"sherinclaudia/sarcastic-comments-on-reddit\")\n",
        "\n",
        "# Headline datasets\n",
        "headlines1 = pd.read_json(f\"{headlinespath}/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "headlines2 = pd.read_json(f\"{headlinespath}/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
        "data_headlines = pd.concat([headlines1, headlines2], ignore_index=True)\n",
        "\n",
        "label_headlines = (\n",
        "    data_headlines\n",
        "    .drop(columns=[\"article_link\"], errors=\"ignore\")\n",
        "    .rename(columns={\"headline\": \"text\", \"is_sarcastic\": \"is_sarcastic\"})\n",
        "    .assign(source=\"headline\")\n",
        ")\n",
        "\n",
        "# Reddit dataset\n",
        "reddit = pd.read_csv(f\"{redditpath}/train-balanced-sarcasm.csv\")\n",
        "\n",
        "#  Reddit: stratified fixed-size sampling (N = 222,664, same as other models)\n",
        "N = 222_664\n",
        "counts = reddit['label'].value_counts()\n",
        "props  = counts / counts.sum()\n",
        "take   = (props * N).round().astype(int)\n",
        "\n",
        "# cap by available rows per label\n",
        "take = take.clip(upper=counts)\n",
        "\n",
        "# fix total to hit N after rounding/clipping\n",
        "diff = N - int(take.sum())\n",
        "\n",
        "if diff > 0:\n",
        "    # add where there's headroom, starting from largest props\n",
        "    order = props.sort_values(ascending=False).index.tolist()\n",
        "    i = 0\n",
        "    while diff > 0 and any((counts - take) > 0):\n",
        "        lbl = order[i % len(order)]\n",
        "        if take[lbl] < counts[lbl]:\n",
        "            take[lbl] += 1\n",
        "            diff -= 1\n",
        "        i += 1\n",
        "elif diff < 0:\n",
        "    # remove starting from smallest props\n",
        "    order = props.sort_values(ascending=True).index.tolist()\n",
        "    i = 0\n",
        "    while diff < 0 and any(take > 0):\n",
        "        lbl = order[i % len(order)]\n",
        "        if take[lbl] > 0:\n",
        "            take[lbl] -= 1\n",
        "            diff += 1\n",
        "        i += 1\n",
        "\n",
        "# sample per label\n",
        "reddit_sample = (\n",
        "    reddit.groupby(\"label\", group_keys=False)\n",
        "          .apply(lambda x: x.sample(n=int(take.loc[x.name]), random_state=42))\n",
        "          .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "label_reddit = (\n",
        "    reddit_sample\n",
        "    .rename(columns={\"comment\": \"text\", \"label\": \"is_sarcastic\"})\n",
        "    .assign(source=\"reddit\")\n",
        "    [[\"text\", \"is_sarcastic\", \"source\"]]\n",
        ")\n",
        "\n",
        "# Twitter dataset (train + test)\n",
        "train_twitter = pd.read_csv(f\"{twitterpath}/train.csv\")\n",
        "test_twitter  = pd.read_csv(f\"{twitterpath}/test.csv\")\n",
        "\n",
        "# Filter out figurative & assign labels\n",
        "def preprocess_twitter(df):\n",
        "    df = df[df[\"class\"] != \"figurative\"].copy()\n",
        "    df[\"is_sarcastic\"] = np.where(df[\"class\"] == \"regular\", 0, 1)\n",
        "    df = df.rename(columns={\"tweets\": \"text\"}).drop(columns=[\"class\"])\n",
        "    df[\"source\"] = \"twitter\"\n",
        "    return df[[\"text\", \"is_sarcastic\", \"source\"]]\n",
        "\n",
        "label_twitter = pd.concat([preprocess_twitter(train_twitter),\n",
        "                           preprocess_twitter(test_twitter)],\n",
        "                          ignore_index=True)\n",
        "\n",
        "# - SPLITS\n",
        "# Train/Val pool = Reddit only\n",
        "trainval_df = label_reddit.copy()\n",
        "\n",
        "# Test set = everything non-Reddit\n",
        "test_df = pd.concat([label_headlines, label_twitter], ignore_index=True)\n",
        "\n",
        "print(\"Train/Val (Reddit) size:\", len(trainval_df))   # should be ~222,664\n",
        "print(\"Test (Non-Reddit) size:\", len(test_df))\n",
        "print(\"Test sources:\\n\", test_df[\"source\"].value_counts())\n",
        "print(\"Train/Val label balance:\\n\", trainval_df[\"is_sarcastic\"].value_counts())\n",
        "\n",
        "# create a stratified train/val split from the Reddit pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(\n",
        "    trainval_df, test_size=0.1, random_state=42, stratify=trainval_df[\"is_sarcastic\"]\n",
        ")\n",
        "print(\"Train size:\", len(train_df), \"Val size:\", len(val_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1605aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c1605aa",
        "outputId": "11cadb23-e2b4-4d5d-ad95-44bc82d3c051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 222664/222664 [00:02<00:00, 87577.59it/s] \n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 121582/121582 [00:03<00:00, 31438.16it/s]\n"
          ]
        }
      ],
      "source": [
        "def clean_text(txt):\n",
        "    if pd.isna(txt):\n",
        "        return np.nan\n",
        "\n",
        "    txt = str(txt)\n",
        "    # Replace URLs\n",
        "    txt = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", txt)\n",
        "    # Replace user mentions\n",
        "    txt = re.sub(r\"@\\w+\", \"<USER>\", txt)\n",
        "    # Remove explicit sarcasm/irony tags (case-insensitive)\n",
        "    txt = re.sub(r\"#\\s*(sarcasm|irony)\\b\", \"\", txt, flags=re.IGNORECASE)\n",
        "    # Remove '#' from hashtags (keep the word)\n",
        "    txt = re.sub(r\"#(\\w+)\", r\"\\1\", txt)\n",
        "    # Normalize whitespace\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "    return txt\n",
        "\n",
        "trainval_df[\"text\"] = trainval_df[\"text\"].progress_apply(clean_text)\n",
        "test_df[\"text\"] = test_df[\"text\"].progress_apply(clean_text)\n",
        "\n",
        "train_df = train_df.rename(columns={\"is_sarcastic\": \"label\"})\n",
        "val_df   = val_df.rename(columns={\"is_sarcastic\": \"label\"})\n",
        "test_df  = test_df.rename(columns={\"is_sarcastic\": \"label\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 4213\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# make sure there are no NaN texts, but keep all rows\n",
        "for _df in (train_df, val_df, test_df):\n",
        "    _df[\"text\"] = _df[\"text\"].fillna(\"<EMPTY>\").astype(str)\n",
        "    _df[\"label\"] = _df[\"label\"].astype(int)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True)).map(tokenize_function, batched=True)\n",
        "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True)).map(tokenize_function, batched=True)\n",
        "test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True)).map(tokenize_function, batched=True)\n",
        "\n",
        "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "train_ds.set_format(type=\"torch\", columns=cols)\n",
        "val_ds.set_format(type=\"torch\", columns=cols)\n",
        "test_ds.set_format(type=\"torch\", columns=cols)\n",
        "\n",
        "# Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "9bd81be2a3934facaf389bdf4d87d585",
            "6d8207c3dda143fdbd452189e041a4b4",
            "da8625813fb94ba7acc017272b79d4f6",
            "8baafdb072d14ce29b891ed36743f8a9",
            "c1cef154daeb4685b56843c866e00e31",
            "ae51d9e4eb7b422cbdcb925e8e40ae83",
            "ec2c9d476c23496aa4cca788ce983d0a",
            "0e2ef303fbab4c07a797e4db1298e47e",
            "67cf53a4c0aa4a1488c814bf8bca2519",
            "e1a7249d3b2b4dcfb5051067415163da",
            "78deff7e980a4b7fa6f975af9bd4dfe0",
            "b53ee972e6f84c2a9fe39faa89f7a128",
            "52a4bdf25df042fd8a4eb836749bd2bb",
            "df25e18ff22244caab8c1332cd0ff046",
            "e0ce6cdad0b548c0af6b12fc7f329dcf",
            "042dd343f3f345b6a0d8da0199eb09cd",
            "e5021542d199416398d121a59f24fd21",
            "94a8d756b1144a7ba381b2b1a15762d7",
            "59e548a437fa4792b559d41c6fe98776",
            "94db7bb9683f4e9d98095c75fb64b883",
            "c187bbc7d6ed414c953679572a06080e",
            "5f2f4fb49b2f48b690f6d879b762a2df",
            "561342afbc594dc7918f0efcc84c60f3",
            "1c5c6f8958dc47a69eb3b899ab198885",
            "423e6c6e54bb4f56a2c996a1d9e4e127",
            "f35d737e09f94676b02418a7f554ac59",
            "e6f64011439e4057a6f3d17b04afc1ce",
            "ad3b460410a548739b2f9edf263c7062",
            "881025503b674919bf34cd6f3c45c048",
            "3fa59215a7c349eda77bf5a93a2bfe07",
            "d421f6cffbdb4058b5e103882ddc7a96",
            "28d8b162f2b84396b62146a4054d7144",
            "13e1bf61bd7f45d7a196f70d161d3b74"
          ]
        },
        "id": "sM9k9Fw31OA6",
        "outputId": "e14cd9d4-d8f6-4f56-8b98-26b292c9e202"
      },
      "id": "sM9k9Fw31OA6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200397 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bd81be2a3934facaf389bdf4d87d585"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/22267 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b53ee972e6f84c2a9fe39faa89f7a128"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/121582 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "561342afbc594dc7918f0efcc84c60f3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna: tune on Reddit train/val\n",
        "def objective(trial):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\", num_labels=2\n",
        "    )\n",
        "\n",
        "    lr = trial.suggest_float(\"lr\", 1e-6, 5e-5, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
        "    num_epochs = trial.suggest_int(\"epochs\", 2, 4)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"results/trial_{trial.number}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=weight_decay,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        logging_dir=\"logs\",\n",
        "        logging_steps=20,\n",
        "        seed=SEED,\n",
        "        disable_tqdm=False,\n",
        "        report_to = 'none'\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    m = trainer.evaluate(eval_dataset=val_ds)\n",
        "    return m[\"eval_f1\"]\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=6)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(f\"  Value (F1): {study.best_trial.value}\")\n",
        "for k, v in study.best_trial.params.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "OCmIZDTI3Jd9",
        "outputId": "0061982e-4314-403b-c2ea-3e96f2cb7a6d"
      },
      "id": "OCmIZDTI3Jd9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-28 08:31:30,341] A new study created in memory with name: no-name-13470b8a-6d33-4b98-920c-ab8f3061e458\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2381166539.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25050' max='25050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25050/25050 1:17:08, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.512100</td>\n",
              "      <td>0.517509</td>\n",
              "      <td>0.742713</td>\n",
              "      <td>0.732377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.509700</td>\n",
              "      <td>0.511315</td>\n",
              "      <td>0.747070</td>\n",
              "      <td>0.737876</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1392' max='1392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1392/1392 01:26]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-28 09:50:07,518] Trial 0 finished with value: 0.7378758261193336 and parameters: {'lr': 4.958037077389076e-06, 'batch_size': 16, 'weight_decay': 0.062739833693706, 'epochs': 2}. Best is trial 0 with value: 0.7378758261193336.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2381166539.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37575' max='37575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37575/37575 1:55:52, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.514700</td>\n",
              "      <td>0.536441</td>\n",
              "      <td>0.729600</td>\n",
              "      <td>0.715682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.546700</td>\n",
              "      <td>0.524463</td>\n",
              "      <td>0.738851</td>\n",
              "      <td>0.725746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.521300</td>\n",
              "      <td>0.523865</td>\n",
              "      <td>0.741860</td>\n",
              "      <td>0.732278</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1392' max='1392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1392/1392 01:26]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-28 11:47:27,844] Trial 1 finished with value: 0.7322775966464835 and parameters: {'lr': 1.8513489811559401e-06, 'batch_size': 16, 'weight_decay': 0.0383230692839596, 'epochs': 3}. Best is trial 0 with value: 0.7378758261193336.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2381166539.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25050' max='25050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25050/25050 1:17:28, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.471500</td>\n",
              "      <td>0.502356</td>\n",
              "      <td>0.754659</td>\n",
              "      <td>0.745753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.447400</td>\n",
              "      <td>0.504935</td>\n",
              "      <td>0.761441</td>\n",
              "      <td>0.757110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1392' max='1392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1392/1392 01:26]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-28 13:06:24,085] Trial 2 finished with value: 0.7571101966163695 and parameters: {'lr': 1.676654209236382e-05, 'batch_size': 16, 'weight_decay': 0.0717726301638075, 'epochs': 2}. Best is trial 2 with value: 0.7571101966163695.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2381166539.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8790' max='50100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8790/50100 25:54 < 2:01:45, 5.65 it/s, Epoch 0.70/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final training with best params\n",
        "best_params = study.best_trial.params\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"full_finetuned_distilbert\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=best_params.get(\"lr\", 3e-5),\n",
        "    per_device_train_batch_size=best_params.get(\"batch_size\", 16),\n",
        "    per_device_eval_batch_size=best_params.get(\"batch_size\", 16),\n",
        "    num_train_epochs=best_params.get(\"epochs\", 3),\n",
        "    weight_decay=best_params.get(\"weight_decay\", 0.01),\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    logging_dir=\"logs_final\",\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\" Starting final full fine-tuning (Reddit train/val)...\")\n",
        "t0 = time.time()\n",
        "trainer.train()\n",
        "train_time = time.time() - t0\n",
        "print(f\" Training completed in {train_time/60:.2f} minutes\")"
      ],
      "metadata": {
        "id": "W-J98Uz33WCH"
      },
      "id": "W-J98Uz33WCH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on Reddit val + Non-Reddit test\n",
        "val_metrics = trainer.evaluate(eval_dataset=val_ds)\n",
        "\n",
        "test_preds = trainer.predict(test_ds)\n",
        "logits = test_preds.predictions\n",
        "labels = test_preds.label_ids\n",
        "\n",
        "def test_metrics_fn(logits, labels):\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds)\n",
        "    try:\n",
        "        probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
        "        roc = roc_auc_score(labels, probs)\n",
        "    except:\n",
        "        roc = float(\"nan\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"roc_auc\": roc}\n",
        "\n",
        "test_metrics = test_metrics_fn(logits, labels)\n",
        "\n",
        "print(\"\\n===== VALIDATION (Reddit) METRICS =====\")\n",
        "for k, v in val_metrics.items():\n",
        "    if k.startswith(\"eval_\"):\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "print(\"\\n===== TEST (Non-Reddit) METRICS =====\")\n",
        "for k, v in test_metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# Artifacts\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# Confusion Matrix (Non-Reddit test)\n",
        "y_true = labels\n",
        "y_pred = np.argmax(logits, axis=-1)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Sarcastic\", \"Sarcastic\"])\n",
        "disp.plot(cmap=\"Blues\", ax=ax, colorbar=False)\n",
        "plt.title(\"Confusion Matrix - Test Set (Non-Reddit)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/confusion_matrix_test.png\", bbox_inches=\"tight\", dpi=300)\n",
        "plt.close(fig)\n",
        "print(\" Confusion matrix saved to results/confusion_matrix_test.png\")\n",
        "\n",
        "# ROC Curve (Non-Reddit test)\n",
        "probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
        "fpr, tpr, thr = roc_curve(y_true, probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
        "ax.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(\"ROC Curve - Test Set (Non-Reddit)\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/roc_curve_test.png\", bbox_inches=\"tight\", dpi=300)\n",
        "plt.close(fig)\n",
        "print(\" ROC curve saved to results/roc_curve_test.png\")"
      ],
      "metadata": {
        "id": "to-dQhL93eZr"
      },
      "id": "to-dQhL93eZr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model_dir = \"results/best_full_finetuned_distilbert\"\n",
        "trainer.save_model(model_dir)\n",
        "pt_path = os.path.join(\"results\", \"best_full_finetuned_distilbert.pt\")\n",
        "torch.save(model.state_dict(), pt_path)\n",
        "print(f\"üíæ Model weights saved as {pt_path}\")"
      ],
      "metadata": {
        "id": "4nzaqlqX3kkB"
      },
      "id": "4nzaqlqX3kkB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary\n",
        "summary = {\n",
        "    \"Best Parameters\": study.best_trial.params,\n",
        "    \"Training Time (min)\": round(train_time / 60, 2),\n",
        "    \"Validation Metrics\": {k: round(v, 4) for k, v in val_metrics.items() if k.startswith(\"eval_\")},\n",
        "    \"Test Metrics\": {k: round(v, 4) for k, v in test_metrics.items()},\n",
        "    \"ROC AUC (Test)\": round(roc_auc, 4)\n",
        "}\n",
        "print(\"\\n===== SUMMARY =====\")\n",
        "for k, v in summary.items():\n",
        "    print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "HPIKNpjW5z8Z"
      },
      "id": "HPIKNpjW5z8Z",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
    
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
