{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94823be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angsp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afcbd2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angsp\\AppData\\Local\\Temp\\ipykernel_32580\\129402213.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  reddit_sample = reddit.groupby(\"label\", group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined size: 222664\n",
      "source\n",
      "reddit      101082\n",
      "twitter      66254\n",
      "headline     55328\n",
      "Name: count, dtype: int64\n",
      "is_sarcastic\n",
      "1    121699\n",
      "0    100965\n",
      "Name: count, dtype: int64\n",
      "(222664, 3)\n",
      "                                                text  is_sarcastic  source\n",
      "0                                                Yes             0  reddit\n",
      "1  Is there a subreddit for innocent thumbnails t...             0  reddit\n",
      "2                                   The truth sadly.             0  reddit\n",
      "3            Yes, all of those countries I remember.             0  reddit\n",
      "4  erm.. \"the more u play the less RNG matters\" ....             0  reddit\n"
     ]
    }
   ],
   "source": [
    "# Load data from kaggle\n",
    "headlinespath = kagglehub.dataset_download(\"rmisra/news-headlines-dataset-for-sarcasm-detection\")\n",
    "twitterpath   = kagglehub.dataset_download(\"nikhiljohnk/tweets-with-sarcasm-and-irony\")\n",
    "redditpath    = kagglehub.dataset_download(\"sherinclaudia/sarcastic-comments-on-reddit\")\n",
    "\n",
    "# Headline datasets\n",
    "headlines1 = pd.read_json(f\"{headlinespath}/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "headlines2 = pd.read_json(f\"{headlinespath}/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "data_headlines = pd.concat([headlines1, headlines2], ignore_index=True)\n",
    "\n",
    "label_headlines = (\n",
    "    data_headlines\n",
    "    .drop(columns=[\"article_link\"], errors=\"ignore\")\n",
    "    .rename(columns={\"headline\": \"text\", \"is_sarcastic\": \"is_sarcastic\"})\n",
    "    .assign(source=\"headline\")\n",
    ")\n",
    "\n",
    "# Reddit dataset\n",
    "reddit = pd.read_csv(f\"{redditpath}/train-balanced-sarcasm.csv\")\n",
    "\n",
    "# Stratified 10% sampling for reddit\n",
    "reddit_sample = reddit.groupby(\"label\", group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=0.10, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "label_reddit = (\n",
    "    reddit_sample\n",
    "    .rename(columns={\"comment\": \"text\", \"label\": \"is_sarcastic\"})\n",
    "    .assign(source=\"reddit\")\n",
    "    [[\"text\", \"is_sarcastic\", \"source\"]]\n",
    ")\n",
    "\n",
    "# Twitter dataset (train + test)\n",
    "train_twitter = pd.read_csv(f\"{twitterpath}/train.csv\")\n",
    "test_twitter  = pd.read_csv(f\"{twitterpath}/test.csv\")\n",
    "\n",
    "# Filter out figurative & assign labels\n",
    "def preprocess_twitter(df):\n",
    "    df = df[df[\"class\"] != \"figurative\"].copy()\n",
    "    df[\"is_sarcastic\"] = np.where(df[\"class\"] == \"regular\", 0, 1)\n",
    "    df = df.rename(columns={\"tweets\": \"text\"}).drop(columns=[\"class\"])\n",
    "    df[\"source\"] = \"twitter\"\n",
    "    return df[[\"text\", \"is_sarcastic\", \"source\"]]\n",
    "\n",
    "label_twitter = pd.concat([preprocess_twitter(train_twitter),\n",
    "                           preprocess_twitter(test_twitter)],\n",
    "                          ignore_index=True)\n",
    "\n",
    "\n",
    "combine_df = pd.concat([label_reddit, label_headlines, label_twitter], ignore_index=True)\n",
    "print(\"Combined size:\", len(combine_df))\n",
    "print(combine_df[\"source\"].value_counts())\n",
    "print(combine_df[\"is_sarcastic\"].value_counts())\n",
    "\n",
    "print(combine_df.shape)\n",
    "print(combine_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1605aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222664/222664 [00:01<00:00, 164336.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned combined dataset to combine_data_clean.csv\n"
     ]
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    \n",
    "    txt = str(txt)\n",
    "    # Replace URLs\n",
    "    txt = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", txt)\n",
    "    # Replace user mentions\n",
    "    txt = re.sub(r\"@\\w+\", \"<USER>\", txt)\n",
    "    # Remove explicit sarcasm/irony tags (case-insensitive)\n",
    "    txt = re.sub(r\"#\\s*(sarcasm|irony)\\b\", \"\", txt, flags=re.IGNORECASE)\n",
    "    # Remove '#' from hashtags (keep the word)\n",
    "    txt = re.sub(r\"#(\\w+)\", r\"\\1\", txt)\n",
    "    # Normalize whitespace\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "\n",
    "combine_df[\"text\"] = combine_df[\"text\"].progress_apply(clean_text)\n",
    "\n",
    "combine_df.to_csv(\"combine_data_clean.csv\", index=False)\n",
    "print(\"Saved cleaned combined dataset to combine_data_clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
