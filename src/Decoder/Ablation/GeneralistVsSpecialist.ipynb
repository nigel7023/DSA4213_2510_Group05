{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59babbdb-fa00-4ffe-97e2-39bfd1be1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer as Trainer\n",
    "from transformers import Seq2SeqTrainingArguments as TrainingArguments\n",
    "import torch\n",
    "import evaluate\n",
    "import gc\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596fd56f-ccbe-4221-9f59-05895351bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\"sarcasm_KD_final.csv\").fillna(\"\")\n",
    "\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=4213)\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42993d6b-62ee-4a58-ae79-afe86501fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_A = (\n",
    "    \"In exactly 1-2 sentences, identify the specific words or phrases that make the text sarcastic \"\n",
    "    \"and explain how they create the sarcastic effect. \"\n",
    "    \"Focus only on observable linguistic elements without adding interpretation beyond what's directly evident in the text.\\n\\n\"\n",
    ")\n",
    "\n",
    "PROMPT_B = (\n",
    "    \"In exactly 1-2 sentences, explain what the speaker actually means by removing the sarcasm \"\n",
    "    \"and stating their true intended message directly. \"\n",
    "    \"Focus on the genuine sentiment or opinion being expressed beneath the sarcastic language.\\n\\n\"\n",
    ")\n",
    "\n",
    "def build_target(row, column_name):\n",
    "    \"\"\"Construct the output text (label) for training.\"\"\"\n",
    "    exp = str(row[column_name]).strip()\n",
    "    return f\"Explanation: {exp}\"\n",
    "\n",
    "# Task A â€” sarcasm cue identification\n",
    "train_df[\"target_text_A\"] = train_df.apply(lambda r: build_target(r, \"part_sarcastic\"), axis=1)\n",
    "val_df[\"target_text_A\"]   = val_df.apply(lambda r: build_target(r, \"part_sarcastic\"), axis=1)\n",
    "\n",
    "# Task B â€” true intent explanation\n",
    "train_df[\"target_text_B\"] = train_df.apply(lambda r: build_target(r, \"sarcasm_explanation\"), axis=1)\n",
    "val_df[\"target_text_B\"]   = val_df.apply(lambda r: build_target(r, \"sarcasm_explanation\"), axis=1)\n",
    "\n",
    "\n",
    "taskA_train_ds = Dataset.from_pandas(train_df[[\"text\", \"target_text_A\"]].rename(columns={\"target_text_A\": \"target_text\"}))\n",
    "taskA_val_ds   = Dataset.from_pandas(val_df[[\"text\", \"target_text_A\"]].rename(columns={\"target_text_A\": \"target_text\"}))\n",
    "\n",
    "taskB_train_ds = Dataset.from_pandas(train_df[[\"text\", \"target_text_B\"]].rename(columns={\"target_text_B\": \"target_text\"}))\n",
    "taskB_val_ds   = Dataset.from_pandas(val_df[[\"text\", \"target_text_B\"]].rename(columns={\"target_text_B\": \"target_text\"}))\n",
    "\n",
    "model_name = \"./flan_t5_full_sarcasm_final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "MAX_SRC_LEN = 128\n",
    "MAX_TGT_LEN = 64\n",
    "\n",
    "def make_preprocess_fn(prompt, tokenizer):\n",
    "    def preprocess(examples):\n",
    "        inputs = [prompt + \"Text: \" + t for t in examples[\"text\"]]\n",
    "        model_inputs = tokenizer(inputs, max_length=MAX_SRC_LEN, truncation=True)\n",
    "        labels = tokenizer(examples[\"target_text\"], max_length=MAX_TGT_LEN, truncation=True)  \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    return preprocess\n",
    "\n",
    "preprocess_A = make_preprocess_fn(PROMPT_A, tokenizer)\n",
    "preprocess_B = make_preprocess_fn(PROMPT_B, tokenizer)\n",
    "\n",
    "taskA_train_tok = taskA_train_ds.map(preprocess_A, batched=True, remove_columns=taskA_train_ds.column_names)\n",
    "taskA_val_tok   = taskA_val_ds.map(preprocess_A,   batched=True, remove_columns=taskA_val_ds.column_names)\n",
    "\n",
    "taskB_train_tok = taskB_train_ds.map(preprocess_B, batched=True, remove_columns=taskB_train_ds.column_names)\n",
    "taskB_val_tok   = taskB_val_ds.map(preprocess_B,   batched=True, remove_columns=taskB_val_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcd1ba-1b7b-4479-90d0-78a6a289bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQUENTIALLY TRAINING GENERALIST\n",
    "\n",
    "import gc,psutil\n",
    "# ============================================================\n",
    "# ðŸ§¹ Memory Cleanup Helper\n",
    "# ============================================================\n",
    "def clear_memory(tag=\"\"):\n",
    "    \"\"\"Clear CUDA + CPU memory to prevent OOM between runs.\"\"\"\n",
    "    print(f\"\\nðŸ§¹ Clearing memory {tag} ...\")\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.synchronize()\n",
    "        mem_alloc = torch.cuda.memory_allocated() / 1024**2\n",
    "        mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        print(f\"   CUDA memory allocated: {mem_alloc:.2f} MB | reserved: {mem_reserved:.2f} MB\")\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"   CPU RSS: {process.memory_info().rss / 1024**2:.2f} MB\\n\")\n",
    "\n",
    "\n",
    "params_path_A = \"./optuna_results/best_t5_taskA_params.json\"\n",
    "with open(params_path_A, \"r\") as f:\n",
    "    bestA = json.load(f)\n",
    "\n",
    "learning_rate_A = bestA[\"learning_rate\"]\n",
    "batch_size_A    = bestA[\"batch_size\"]\n",
    "dropout_rate_A  = bestA[\"dropout_rate\"]\n",
    "weight_decay_A  = bestA[\"weight_decay\"]\n",
    "warmup_ratio_A  = bestA[\"warmup_ratio\"]\n",
    "\n",
    "print(\"Loaded Task A tuned params:\")\n",
    "print(json.dumps(bestA, indent=4))\n",
    "\n",
    "params_path_B = \"./optuna_results/best_t5_taskB_params.json\"\n",
    "with open(params_path_B, \"r\") as f:\n",
    "    bestB = json.load(f)\n",
    "\n",
    "learning_rate_B = bestB[\"learning_rate\"]\n",
    "batch_size_B    = bestB[\"batch_size\"]\n",
    "dropout_rate_B  = bestB[\"dropout_rate\"]\n",
    "weight_decay_B  = bestB[\"weight_decay\"]\n",
    "warmup_ratio_B  = bestB[\"warmup_ratio\"]\n",
    "\n",
    "print(\"Loaded Task B tuned params:\")\n",
    "print(json.dumps(bestB, indent=4))\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "model_name = \"./flan_t5_full_sarcasm_final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    preds = preds[0] if isinstance(preds, tuple) else preds\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    score = rouge.compute(predictions=pred_texts, references=label_texts)\n",
    "    return {\"rougeL\": round(score[\"rougeL\"], 4)}\n",
    "\n",
    "\n",
    "modelA = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "modelA.config.dropout_rate = dropout_rate_A\n",
    "modelA.config.attention_dropout_rate = dropout_rate_A\n",
    "\n",
    "collatorA = DataCollatorForSeq2Seq(tokenizer, model=modelA)\n",
    "\n",
    "argsA = Seq2SeqTrainingArguments(\n",
    "    learning_rate=learning_rate_A,\n",
    "    per_device_train_batch_size=batch_size_A,\n",
    "    per_device_eval_batch_size=max(2, batch_size_A // 2), #prevent OOM\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=weight_decay_A,\n",
    "    warmup_ratio=warmup_ratio_A,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainerA = Seq2SeqTrainer(\n",
    "    model=modelA,\n",
    "    args=argsA,\n",
    "    train_dataset=taskA_train_tok,\n",
    "    eval_dataset=taskA_val_tok,\n",
    "    data_collator=collatorA,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\" Starting fine-tuning on Task A...\")\n",
    "trainerA.train()\n",
    "trainerA.save_model(\"./model_final_taskA\")\n",
    "\n",
    "del modelA, trainerA, collatorA, argsA\n",
    "clear_memory(\"(after Task A)\")\n",
    "\n",
    "\n",
    "modelB = AutoModelForSeq2SeqLM.from_pretrained(\"./model_final_taskA\")\n",
    "modelB.config.dropout_rate = dropout_rate_B\n",
    "modelB.config.attention_dropout_rate = dropout_rate_B\n",
    "\n",
    "collatorB = DataCollatorForSeq2Seq(tokenizer, model=modelB)\n",
    "\n",
    "argsB = Seq2SeqTrainingArguments(\n",
    "    learning_rate=learning_rate_B,\n",
    "    per_device_train_batch_size=batch_size_B,\n",
    "    per_device_eval_batch_size=max(2, batch_size_B // 2), #prevent OOM\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=weight_decay_B,\n",
    "    warmup_ratio=warmup_ratio_B,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainerB = Seq2SeqTrainer(\n",
    "    model=modelB,\n",
    "    args=argsB,\n",
    "    train_dataset=taskB_train_tok,\n",
    "    eval_dataset=taskB_val_tok,\n",
    "    data_collator=collatorB,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Continuing fine-tuning on Task B...\")\n",
    "trainerB.train()\n",
    "trainerB.save_model(\"./model_final_dualtask\")\n",
    "\n",
    "del modelB, trainerB, collatorB, argsB\n",
    "clear_memory(\"(after Task B)\")\n",
    "\n",
    "print(\"Training complete. Sequential Generalist Model saved at ./model_final_dualtask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc1f1a-54af-4c54-a2f7-d62d3aeca699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 â€” Joint Learner + Generator Training\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "params_path_A = \"./optuna_results/best_t5_taskA_params.json\"\n",
    "with open(params_path_A, \"r\") as f:\n",
    "    bestA = json.load(f)\n",
    "\n",
    "learning_rate_A = bestA[\"learning_rate\"]\n",
    "batch_size_A    = bestA[\"batch_size\"]\n",
    "dropout_rate_A  = bestA[\"dropout_rate\"]\n",
    "weight_decay_A  = bestA[\"weight_decay\"]\n",
    "warmup_ratio_A  = bestA[\"warmup_ratio\"]\n",
    "\n",
    "print(\"Loaded Task A tuned params:\")\n",
    "print(json.dumps(bestA, indent=4))\n",
    "\n",
    "params_path_B = \"./optuna_results/best_t5_taskB_params.json\"\n",
    "with open(params_path_B, \"r\") as f:\n",
    "    bestB = json.load(f)\n",
    "\n",
    "learning_rate_B = bestB[\"learning_rate\"]\n",
    "batch_size_B    = bestB[\"batch_size\"]\n",
    "dropout_rate_B  = bestB[\"dropout_rate\"]\n",
    "weight_decay_B  = bestB[\"weight_decay\"]\n",
    "warmup_ratio_B  = bestB[\"warmup_ratio\"]\n",
    "\n",
    "print(\"Loaded Task B tuned params:\")\n",
    "print(json.dumps(bestB, indent=4))\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "model_name = \"./flan_t5_full_sarcasm_final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    preds = preds[0] if isinstance(preds, tuple) else preds\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    score = rouge.compute(predictions=pred_texts, references=label_texts)\n",
    "    return {\"rougeL\": round(score[\"rougeL\"], 4)}\n",
    "\n",
    "# Base model (acts as learner)\n",
    "modelA = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "modelA.config.dropout_rate = dropout_rate_A\n",
    "modelA.config.attention_dropout_rate = dropout_rate_A\n",
    "\n",
    "# Clone it as generator (separate optimizer)\n",
    "generatorA = deepcopy(modelA)\n",
    "\n",
    "collatorA = DataCollatorForSeq2Seq(tokenizer, model=modelA)\n",
    "\n",
    "argsA = Seq2SeqTrainingArguments(\n",
    "    learning_rate=learning_rate_A,\n",
    "    per_device_train_batch_size=batch_size_A,\n",
    "    per_device_eval_batch_size=max(2, batch_size_A // 2),\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=weight_decay_A,\n",
    "    warmup_ratio=warmup_ratio_A,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer for the learner\n",
    "trainerA = Seq2SeqTrainer(\n",
    "    model=modelA,\n",
    "    args=argsA,\n",
    "    train_dataset=taskA_train_tok,\n",
    "    eval_dataset=taskA_val_tok,\n",
    "    data_collator=collatorA,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Trainer for generator â€” self-supervised reconstruction\n",
    "trainerG = Seq2SeqTrainer(\n",
    "    model=generatorA,\n",
    "    args=deepcopy(argsA),\n",
    "    train_dataset=taskA_train_tok,\n",
    "    eval_dataset=taskA_val_tok,\n",
    "    data_collator=collatorA,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Joint training learner + generator on Task A...\")\n",
    "trainerA.train()\n",
    "trainerG.train()\n",
    "\n",
    "# Save both models\n",
    "trainerA.save_model(\"./model_final_taskA_learner\")\n",
    "trainerG.save_model(\"./model_final_taskA_generator\")\n",
    "\n",
    "\n",
    "learnerB = AutoModelForSeq2SeqLM.from_pretrained(\"./model_final_taskA_learner\")\n",
    "generatorB = AutoModelForSeq2SeqLM.from_pretrained(\"./model_final_taskA_generator\")\n",
    "\n",
    "\n",
    "def generate_taskA_examples(generator, tokenizer, raw_dataset, num_samples=200):\n",
    "    \"\"\"Generate replay samples from raw Task A validation data.\"\"\"\n",
    "    generator = generator.to(\"cuda\")\n",
    "    generator.eval()\n",
    "\n",
    "    # Select subset\n",
    "    sample_data = raw_dataset.select(range(min(num_samples, len(raw_dataset))))\n",
    "\n",
    "    # Use the original Task A prompt\n",
    "    prompts = [\n",
    "        \"In exactly 1â€“2 sentences, identify the specific words or phrases \"\n",
    "        \"that make the text sarcastic and explain how they create the sarcastic effect. \"\n",
    "        \"Focus only on observable linguistic elements without adding interpretation beyond what's directly evident in the text.\\n\\n\"\n",
    "        f\"Text: {x}\"\n",
    "        for x in sample_data[\"text\"]\n",
    "    ]\n",
    "\n",
    "    # Tokenize + generate outputs\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = generator.generate(**inputs, max_new_tokens=64)\n",
    "    gens = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Return replay samples as dictionaries\n",
    "    replay_data = [{\"text\": p, \"target_text\": g} for p, g in zip(prompts, gens)]\n",
    "    return replay_data\n",
    "\n",
    "print(\"Generating replay samples from Task A...\")\n",
    "replayA = generate_taskA_examples(generatorB, tokenizer, taskA_val_ds, num_samples=200)\n",
    "\n",
    "\n",
    "replayA_ds = Dataset.from_list(replayA)\n",
    "replayA_tok = replayA_ds.map(\n",
    "    preprocess_A,\n",
    "    batched=True,\n",
    "    remove_columns=replayA_ds.column_names\n",
    ")\n",
    "\n",
    "\n",
    "combined_train = concatenate_datasets([taskB_train_tok, replayA_tok])\n",
    "combined_val   = concatenate_datasets([taskB_val_tok, replayA_tok])  # optional validation replay\n",
    "\n",
    "print(f\"Combined training size: {len(combined_train)} samples\")\n",
    "\n",
    "\n",
    "collatorB = DataCollatorForSeq2Seq(tokenizer, model=learnerB)\n",
    "\n",
    "argsB = Seq2SeqTrainingArguments(\n",
    "    learning_rate=learning_rate_B,\n",
    "    per_device_train_batch_size=batch_size_B,\n",
    "    per_device_eval_batch_size=max(2, batch_size_B // 2),\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=weight_decay_B,\n",
    "    warmup_ratio=warmup_ratio_B,\n",
    "    eval_strategy=\"epoch\",     \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainerB = Seq2SeqTrainer(\n",
    "    model=learnerB,\n",
    "    args=argsB,\n",
    "    train_dataset=combined_train,\n",
    "    eval_dataset=combined_val,\n",
    "    data_collator=collatorB,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainerB.train()\n",
    "trainerB.save_model(\"./model_final_lamol\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208645c6-ee76-43d8-a94c-983f8f546d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation for Task A (cue task)\n",
    "\n",
    "specilist_cue = \"./model_final_taskA\"\n",
    "generalist_seq = \"./model_final_dualtask\"  \n",
    "generalist_lamol = \"./model_final_lamol\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(specilist_cue)\n",
    "\n",
    "specialist_model = AutoModelForSeq2SeqLM.from_pretrained(specilist_cue).to(device)\n",
    "generalist_seq_model = AutoModelForSeq2SeqLM.from_pretrained(generalist_seq).to(device)\n",
    "generalist_lamol_model = AutoModelForSeq2SeqLM.from_pretrained(generalist_lamol).to(device)\n",
    "\n",
    "\n",
    "def generate_response(model, sentence):\n",
    "    prompt = (\n",
    "        \"In exactly 1-2 sentences, identify the specific words or phrases \"\n",
    "        \"that make the text sarcastic and explain how they create the sarcastic effect. \"\n",
    "        \"Focus only on observable linguistic elements without adding interpretation \"\n",
    "        \"beyond what's directly evident in the text.\"\n",
    "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        do_sample=True,\n",
    "        temperature=2.3,       \n",
    "        top_p=0.6,            \n",
    "        top_k=60,\n",
    "        num_beams=10,           \n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.4,  \n",
    "        length_penalty=1.0,\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    text = re.sub(r\"^(Explanation|Answer|Response)\\s*:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"Yay my shoe broke!\",\n",
    "    \"If the shooter shouldn't have been able to get a gun, the solution is obviously more guns, right? <URL>\",\n",
    "    \"baby feels foolish after realizing stranger waving at toddler next seat over\",\n",
    "    \"Wonderful, traffic is even worse than yesterday!\",\n",
    "    \"I'm so sorry I can't read sarcasm over the internet\",\n",
    "    \"Crying before I go into work... This is going to be a great night. #Sarcasm #WishItWasTrue\",\n",
    "    \"In a cab on the way home from the airport. What a long day. Work tomorrow is going to be AMAZING. Should be home by 3AM.\",\n",
    "    \"hey <user> thanks for making it easy for me to take my music with me . # ihateyourupdates\",\n",
    "    \"It could confuse your muscles and make muscle grow in places where you didn't actually work out.\",\n",
    "    \"Yay, 2-hour traffic for a 10-minute errand. Exactly what I needed ðŸ™ƒ\",\n",
    "    \"This guy gets a gold star for such excellent parking in the handicap lot!\",\n",
    "    \"How else will we feel superior if not by our amazing taste in phones?\",\n",
    "    \"Received a compliment today that I look very relaxed. If only this person knew just how much effort it takes to look this relaxed.\",\n",
    "    \"My phone dying at 5% is the highlight of my day.\",\n",
    "    \"overweight man repeatedly introduced to overweight woman at party\",\n",
    "    \"How dare you type out Obama's name and not praise him you racist\",\n",
    "    \"No, it's perfectly safe for nurses to shove pills in your mouth without any education\",\n",
    "    \"Great, another inspirational quote on LinkedIn. Just what I needed.\",\n",
    "    \"even aside from the blatant misogyny, this is great because we have so much space in our prisons!\",\n",
    "    \"DSA4213 is sooo easy even my grandma can score A+\"\n",
    "    \"Trains are delayed on both directions. Instead of seeing people rushing to take the bus or cab, they were taking pictures. Haha..\",\n",
    "    \"I have never felt more alive than during DSA4213 finetuning, nothing like a few CUDA OOMs to keep the adrenaline going.\",\n",
    "    \"I love how DSA4213 keeps me humble, every single assignment and quizzes\",\n",
    "    \"DSA4213 is not that hard, I just needed 4 GPUs and divine intervention\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    s_out = generate_response(specialist_model, s)\n",
    "    gseq_out = generate_response(generalist_seq_model, s)\n",
    "    gl_out = generate_response(generalist_lamol_model, s)\n",
    "    \n",
    "    print(f\"\\n Sentence: {s}\")\n",
    "    print(f\"Cue task Specialist: {s_out}\")\n",
    "    print(f\"Generalist Seq: {gseq_out}\")\n",
    "    print(f\"Generalist LAMOL: {gl_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be84f0b0-b793-4bb3-8a79-248878c5ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation for Task B (Explain task)\n",
    "\n",
    "specilist_explain = \"./model_final_taskB\"\n",
    "generalist_seq = \"./model_final_dualtask\" \n",
    "generalist_lamol = \"./model_final_lamol\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(specilist_cue)\n",
    "\n",
    "specialist_model = AutoModelForSeq2SeqLM.from_pretrained(specilist_explain).to(device)\n",
    "generalist_seq_model = AutoModelForSeq2SeqLM.from_pretrained(generalist_seq).to(device)\n",
    "generalist_lamol_model = AutoModelForSeq2SeqLM.from_pretrained(generalist_lamol).to(device)\n",
    "\n",
    "\n",
    "\n",
    "def generate_response(model, sentence):\n",
    "    prompt = (\n",
    "        \"In exactly 1-2 sentences, explain what the speaker actually means by removing the sarcasm \"\n",
    "        \"and stating their true intended message directly. \"\n",
    "        \"Focus on the genuine sentiment or opinion being expressed beneath the sarcastic language.\\n\\n\"\n",
    "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=True,\n",
    "            temperature=2.3,       \n",
    "            top_p=0.6,          \n",
    "            top_k=60,\n",
    "            num_beams=10,          \n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.4, \n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    text = re.sub(r\"^(Explanation|Answer|Response)\\s*:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"Yay my shoe broke!\",\n",
    "    \"If the shooter shouldn't have been able to get a gun, the solution is obviously more guns, right? <URL>\",\n",
    "    \"baby feels foolish after realizing stranger waving at toddler next seat over\",\n",
    "    \"Wonderful, traffic is even worse than yesterday!\",\n",
    "    \"I'm so sorry I can't read sarcasm over the internet\",\n",
    "    \"Crying before I go into work... This is going to be a great night. #Sarcasm #WishItWasTrue\",\n",
    "    \"In a cab on the way home from the airport. What a long day. Work tomorrow is going to be AMAZING. Should be home by 3AM.\",\n",
    "    \"hey <user> thanks for making it easy for me to take my music with me . # ihateyourupdates\",\n",
    "    \"It could confuse your muscles and make muscle grow in places where you didn't actually work out.\",\n",
    "    \"Yay, 2-hour traffic for a 10-minute errand. Exactly what I needed ðŸ™ƒ\",\n",
    "    \"This guy gets a gold star for such excellent parking in the handicap lot!\",\n",
    "    \"How else will we feel superior if not by our amazing taste in phones?\",\n",
    "    \"Received a compliment today that I look very relaxed. If only this person knew just how much effort it takes to look this relaxed.\",\n",
    "    \"My phone dying at 5% is the highlight of my day.\",\n",
    "    \"overweight man repeatedly introduced to overweight woman at party\",\n",
    "    \"How dare you type out Obama's name and not praise him you racist\",\n",
    "    \"No, it's perfectly safe for nurses to shove pills in your mouth without any education\",\n",
    "    \"Great, another inspirational quote on LinkedIn. Just what I needed.\",\n",
    "    \"even aside from the blatant misogyny, this is great because we have so much space in our prisons!\",\n",
    "    \"DSA4213 is sooo easy even my grandma can score A+\"\n",
    "    \"Trains are delayed on both directions. Instead of seeing people rushing to take the bus or cab, they were taking pictures. Haha..\",\n",
    "    \"I have never felt more alive than during DSA4213 finetuning, nothing like a few CUDA OOMs to keep the adrenaline going.\",\n",
    "    \"I love how DSA4213 keeps me humble, every single assignment and quizzes\",\n",
    "    \"DSA4213 is not that hard, I just needed 4 GPUs and divine intervention\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    s_out = generate_response(specialist_model, s)\n",
    "    gseq_out = generate_response(generalist_seq_model, s)\n",
    "    gl_out = generate_response(generalist_lamol_model, s)\n",
    "    \n",
    "    print(f\"\\n Sentence: {s}\")\n",
    "    print(f\"Explain task Specialist: {s_out}\")\n",
    "    print(f\"Generalist Seq: {gseq_out}\")\n",
    "    print(f\"Generalist LAMOL: {gl_out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
