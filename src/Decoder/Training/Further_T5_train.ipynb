{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e27204-073c-480d-91aa-568f48774878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc, psutil\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import evaluate\n",
    "import optuna\n",
    "from optuna.exceptions import TrialPruned\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c660cd-b284-4005-b399-4d0e94644351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1Ô∏è‚É£ Load dataset\n",
    "data = pd.read_csv(\"sarcasm_KD_final.csv\").fillna(\"\")\n",
    "\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=4213)\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a9d5e-3195-463f-9481-fa84324421e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_A = (\n",
    "    \"In exactly 1-2 sentences, identify the specific words or phrases that make the text sarcastic \"\n",
    "    \"and explain how they create the sarcastic effect. \"\n",
    "    \"Focus only on observable linguistic elements without adding interpretation beyond what's directly evident in the text.\\n\\n\"\n",
    ")\n",
    "\n",
    "PROMPT_B = (\n",
    "    \"In exactly 1-2 sentences, explain what the speaker actually means by removing the sarcasm \"\n",
    "    \"and stating their true intended message directly. \"\n",
    "    \"Focus on the genuine sentiment or opinion being expressed beneath the sarcastic language.\\n\\n\"\n",
    ")\n",
    "\n",
    "def build_target(row, column_name):\n",
    "    \"\"\"Construct the output text (label) for training.\"\"\"\n",
    "    exp = str(row[column_name]).strip()\n",
    "    return f\"Explanation: {exp}\"\n",
    "\n",
    "# Task A ‚Äî sarcasm cue identification\n",
    "train_df[\"target_text_A\"] = train_df.apply(lambda r: build_target(r, \"part_sarcastic\"), axis=1)\n",
    "val_df[\"target_text_A\"]   = val_df.apply(lambda r: build_target(r, \"part_sarcastic\"), axis=1)\n",
    "\n",
    "# Task B ‚Äî true intent explanation\n",
    "train_df[\"target_text_B\"] = train_df.apply(lambda r: build_target(r, \"sarcasm_explanation\"), axis=1)\n",
    "val_df[\"target_text_B\"]   = val_df.apply(lambda r: build_target(r, \"sarcasm_explanation\"), axis=1)\n",
    "\n",
    "\n",
    "taskA_train_ds = Dataset.from_pandas(train_df[[\"text\", \"target_text_A\"]].rename(columns={\"target_text_A\": \"target_text\"}))\n",
    "taskA_val_ds   = Dataset.from_pandas(val_df[[\"text\", \"target_text_A\"]].rename(columns={\"target_text_A\": \"target_text\"}))\n",
    "\n",
    "taskB_train_ds = Dataset.from_pandas(train_df[[\"text\", \"target_text_B\"]].rename(columns={\"target_text_B\": \"target_text\"}))\n",
    "taskB_val_ds   = Dataset.from_pandas(val_df[[\"text\", \"target_text_B\"]].rename(columns={\"target_text_B\": \"target_text\"}))\n",
    "\n",
    "model_name = \"./flan_t5_full_sarcasm_final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "MAX_SRC_LEN = 128\n",
    "MAX_TGT_LEN = 64\n",
    "\n",
    "def make_preprocess_fn(prompt, tokenizer):\n",
    "    def preprocess(examples):\n",
    "        inputs = [prompt + \"Text: \" + t for t in examples[\"text\"]]  \n",
    "        model_inputs = tokenizer(inputs, max_length=MAX_SRC_LEN, truncation=True)\n",
    "        labels = tokenizer(examples[\"target_text\"], max_length=MAX_TGT_LEN, truncation=True)  \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    return preprocess\n",
    "\n",
    "preprocess_A = make_preprocess_fn(PROMPT_A, tokenizer)\n",
    "preprocess_B = make_preprocess_fn(PROMPT_B, tokenizer)\n",
    "\n",
    "taskA_train_tok = taskA_train_ds.map(preprocess_A, batched=True, remove_columns=taskA_train_ds.column_names)\n",
    "taskA_val_tok   = taskA_val_ds.map(preprocess_A,   batched=True, remove_columns=taskA_val_ds.column_names)\n",
    "\n",
    "taskB_train_tok = taskB_train_ds.map(preprocess_B, batched=True, remove_columns=taskB_train_ds.column_names)\n",
    "taskB_val_tok   = taskB_val_ds.map(preprocess_B,   batched=True, remove_columns=taskB_val_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f4f5e-2fc5-4d03-824c-bbd703a7d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, optuna, numpy as np, evaluate, torch\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "def subset_dataset(ds, fraction=0.2, seed=4213):\n",
    "    total = len(ds)\n",
    "    subset_size = int(total * fraction)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(total, subset_size, replace=False)\n",
    "    return ds.select(indices.tolist())\n",
    "\n",
    "# Create smaller subsets for both tasks\n",
    "taskA_train_tok_sub = subset_dataset(taskA_train_tok)\n",
    "taskA_val_tok_sub   = subset_dataset(taskA_val_tok)\n",
    "taskB_train_tok_sub = subset_dataset(taskB_train_tok)\n",
    "taskB_val_tok_sub   = subset_dataset(taskB_val_tok)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    rouge_result = rouge.compute(predictions=pred_texts, references=label_texts)\n",
    "    return {\"rougeL\": round(rouge_result[\"rougeL\"], 4)}\n",
    "\n",
    "\n",
    "def make_objective(train_ds, val_ds, model_name):\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-5, 3e-5, 1e-4, 3e-4])\n",
    "        batch_size    = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "        dropout_rate  = trial.suggest_categorical(\"dropout_rate\", [0.1, 0.2, 0.3])\n",
    "        weight_decay  = trial.suggest_categorical(\"weight_decay\", [0.0, 0.01, 0.05])\n",
    "        warmup_ratio  = trial.suggest_categorical(\"warmup_ratio\", [0.03, 0.06, 0.1])\n",
    "\n",
    "        try:\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "            model.config.dropout_rate = dropout_rate\n",
    "            model.config.attention_dropout_rate = dropout_rate\n",
    "            model.to(\"cpu\")\n",
    "\n",
    "            collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "            training_args = Seq2SeqTrainingArguments(\n",
    "                learning_rate=learning_rate,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=max(2, batch_size // 2),\n",
    "                num_train_epochs=1,  \n",
    "                weight_decay=weight_decay,\n",
    "                warmup_ratio=warmup_ratio,\n",
    "                eval_strategy=\"epoch\",\n",
    "                save_strategy=\"no\",\n",
    "                logging_strategy=\"epoch\",\n",
    "                predict_with_generate=True,\n",
    "                gradient_accumulation_steps=max(1, 32 // batch_size),\n",
    "                fp16=False,\n",
    "                report_to=\"none\",\n",
    "            )\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_ds,\n",
    "                eval_dataset=val_ds,\n",
    "                data_collator=collator,\n",
    "                tokenizer=tokenizer,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "            eval_results = trainer.evaluate()\n",
    "            return eval_results.get(\"eval_rougeL\", 0.0)\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                raise optuna.TrialPruned()\n",
    "            else:\n",
    "                raise e\n",
    "    return objective\n",
    "\n",
    "\n",
    "save_dir = \"./optuna_results\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\n Running Optuna tuning for Task A (sarcastic cue identification)...\")\n",
    "studyA = optuna.create_study(direction=\"maximize\", study_name=\"t5_taskA_optuna\")\n",
    "studyA.optimize(make_objective(taskA_train_tok_sub, taskA_val_tok_sub, model_name), n_trials=6)\n",
    "\n",
    "bestA = studyA.best_trial\n",
    "bestA_params = bestA.params\n",
    "bestA_params[\"best_rougeL\"] = bestA.value\n",
    "\n",
    "json_path_A = os.path.join(save_dir, \"best_t5_taskA_params.json\")\n",
    "with open(json_path_A, \"w\") as f:\n",
    "    json.dump(bestA_params, f, indent=4)\n",
    "print(f\" Task A best params saved to {json_path_A}\\n\", json.dumps(bestA_params, indent=4))\n",
    "\n",
    "# ----- Task B -----\n",
    "print(\"\\n Running Optuna tuning for Task B (true intent explanation)...\")\n",
    "studyB = optuna.create_study(direction=\"maximize\", study_name=\"t5_taskB_optuna\")\n",
    "studyB.optimize(make_objective(taskB_train_tok_sub, taskB_val_tok_sub, model_name), n_trials=6)\n",
    "\n",
    "bestB = studyB.best_trial\n",
    "bestB_params = bestB.params\n",
    "bestB_params[\"best_rougeL\"] = bestB.value\n",
    "\n",
    "json_path_B = os.path.join(save_dir, \"best_t5_taskB_params.json\")\n",
    "with open(json_path_B, \"w\") as f:\n",
    "    json.dump(bestB_params, f, indent=4)\n",
    "print(f\" Task B best params saved to {json_path_B}\\n\", json.dumps(bestB_params, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31980e70-6d95-4ad8-9d8f-69633134ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory(tag=\"\"):\n",
    "    \"\"\"Clear CUDA + CPU memory to prevent OOM between runs.\"\"\"\n",
    "    print(f\"\\n Clearing memory {tag} ...\")\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.synchronize()\n",
    "        mem_alloc = torch.cuda.memory_allocated() / 1024**2\n",
    "        mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        print(f\"   CUDA memory allocated: {mem_alloc:.2f} MB | reserved: {mem_reserved:.2f} MB\")\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"   CPU RSS: {process.memory_info().rss / 1024**2:.2f} MB\\n\")\n",
    "\n",
    "\n",
    "params_path_A = \"./optuna_results/best_t5_taskA_params.json\"\n",
    "with open(params_path_A, \"r\") as f:\n",
    "    bestA = json.load(f)\n",
    "\n",
    "learning_rate_A = bestA[\"learning_rate\"]\n",
    "batch_size_A    = bestA[\"batch_size\"]\n",
    "dropout_rate_A  = bestA[\"dropout_rate\"]\n",
    "weight_decay_A  = bestA[\"weight_decay\"]\n",
    "warmup_ratio_A  = bestA[\"warmup_ratio\"]\n",
    "\n",
    "print(\" Loaded Task A tuned params:\")\n",
    "print(json.dumps(bestA, indent=4))\n",
    "\n",
    "params_path_B = \"./optuna_results/best_t5_taskB_params.json\"\n",
    "with open(params_path_B, \"r\") as f:\n",
    "    bestB = json.load(f)\n",
    "\n",
    "learning_rate_B = bestB[\"learning_rate\"]\n",
    "batch_size_B    = bestB[\"batch_size\"]\n",
    "dropout_rate_B  = bestB[\"dropout_rate\"]\n",
    "weight_decay_B  = bestB[\"weight_decay\"]\n",
    "warmup_ratio_B  = bestB[\"warmup_ratio\"]\n",
    "\n",
    "print(\" Loaded Task B tuned params:\")\n",
    "print(json.dumps(bestB, indent=4))\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "model_name = \"./flan_t5_full_sarcasm_final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    preds = preds[0] if isinstance(preds, tuple) else preds\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    score = rouge.compute(predictions=pred_texts, references=label_texts)\n",
    "    return {\"rougeL\": round(score[\"rougeL\"], 4)}\n",
    "\n",
    "\n",
    "# Train on sarcastic cue identification\n",
    "modelA = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "modelA.config.dropout_rate = dropout_rate_A\n",
    "modelA.config.attention_dropout_rate = dropout_rate_A\n",
    "\n",
    "collatorA = DataCollatorForSeq2Seq(tokenizer, model=modelA)\n",
    "\n",
    "argsA = Seq2SeqTrainingArguments(\n",
    "    learning_rate=learning_rate_A,\n",
    "    per_device_train_batch_size=batch_size_A,\n",
    "    per_device_eval_batch_size=max(2, batch_size_A // 2),\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=weight_decay_A,\n",
    "    warmup_ratio=warmup_ratio_A,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainerA = Seq2SeqTrainer(\n",
    "    model=modelA,\n",
    "    args=argsA,\n",
    "    train_dataset=taskA_train_tok,\n",
    "    eval_dataset=taskA_val_tok,\n",
    "    data_collator=collatorA,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\" Starting fine-tuning on Task A...\")\n",
    "trainerA.train()\n",
    "trainerA.save_model(\"./model_final_taskA\")\n",
    "\n",
    "del modelA, trainerA, collatorA, argsA\n",
    "clear_memory(\"(after Task A)\")\n",
    "\n",
    "# Fine-tuning on true intent explanation\n",
    "modelB = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "modelB.config.dropout_rate = dropout_rate_B\n",
    "modelB.config.attention_dropout_rate = dropout_rate_B\n",
    "\n",
    "collatorB = DataCollatorForSeq2Seq(tokenizer, model=modelB)\n",
    "\n",
    "argsB = Seq2SeqTrainingArguments(\n",
    "    learning_rate=learning_rate_B,\n",
    "    per_device_train_batch_size=batch_size_B,\n",
    "    per_device_eval_batch_size=max(2, batch_size_B // 2),\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=weight_decay_B,\n",
    "    warmup_ratio=warmup_ratio_B,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainerB = Seq2SeqTrainer(\n",
    "    model=modelB,\n",
    "    args=argsB,\n",
    "    train_dataset=taskB_train_tok,\n",
    "    eval_dataset=taskB_val_tok,\n",
    "    data_collator=collatorB,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\" Fine-tuning on Task B...\")\n",
    "trainerB.train()\n",
    "trainerB.save_model(\"./model_final_taskB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f08284-c1aa-4295-bb76-d1bd738e4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA_name = \"./model_final_taskA\"\n",
    "base_name = \"google/flan-t5-base\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelA_name)\n",
    "\n",
    "\n",
    "modelA = AutoModelForSeq2SeqLM.from_pretrained(modelA_name).to(device)\n",
    "modelbase = AutoModelForSeq2SeqLM.from_pretrained(base_name).to(device)\n",
    "\n",
    "\n",
    "def generate_response(model, sentence):\n",
    "    prompt = (\n",
    "        \"In exactly 1-2 sentences, identify the specific words or phrases \"\n",
    "        \"that make the text sarcastic and explain how they create the sarcastic effect. \"\n",
    "        \"Focus only on observable linguistic elements without adding interpretation \"\n",
    "        \"beyond what's directly evident in the text.\"\n",
    "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=True,\n",
    "            temperature=2.3,       \n",
    "            top_p=0.6,            \n",
    "            top_k=60,\n",
    "            num_beams=10,           \n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.4,  \n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    text = re.sub(r\"^(Explanation|Answer|Response)\\s*:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3Ô∏è‚É£ Compare models\n",
    "# -------------------------------------------------\n",
    "sentences = [\n",
    "    \"Yeah, join the military and help the empire spread destruction across the world in the name of human rights, get PTSD, become homeless, sounds like a great way to improve one's situation.\",\n",
    "    \"Sorry, I'm too busy eating lobster in my yacht (paid for with self-published comics profits) to comment. <USER> <USER>\",\n",
    "    \"Oh perfect, the fire alarm goes off right when I start my presentation.\",\n",
    "    \"I just love when my boss schedules a meeting during lunch.\",\n",
    "    \"Oh great, the printer jammed right before the deadline.\",\n",
    "    \"Wonderful, traffic is even worse than yesterday!\",\n",
    "    \"Yeah, because everyone totally loves working overtime for free.\",\n",
    "    \"Crying before I go into work... This is going to be a great night. #Sarcasm #WishItWasTrue\",\n",
    "    \"Oh sure, because staying up till 3am totally helps with productivity üòí\",\n",
    "    \"hey <user> thanks for making it easy for me to take my music with me . # ihateyourupdates\",\n",
    "    \"It could confuse your muscles and make muscle grow in places where you didn't actually work out.\",\n",
    "    \"Yay, 2-hour traffic for a 10-minute errand. Exactly what I needed üôÉ\",\n",
    "    \"This guy gets a gold star for such excellent parking in the handicap lot!\",\n",
    "    \"How else will we feel superior if not by our amazing taste in phones?\",\n",
    "    \"Guess I‚Äôll just refresh the page for the 20th time. Maybe that‚Äôll fix it ü§°\",\n",
    "    \"My phone dying at 5% is the highlight of my day.\",\n",
    "    \"parrot's previous owner obviously watched a lot of the price is right\",\n",
    "    \"Oh totally, I love when people reply ‚Äòk‚Äô to my long texts.\",\n",
    "    \"Gotta save our children from the dangers of text on a screen in a rhythm game\",\n",
    "    \"Great, another inspirational quote on LinkedIn. Just what I needed.\",\n",
    "    \"even aside from the blatant misogyny, this is great because we have so much space in our prisons!\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    A_out = generate_response(modelA, s)\n",
    "    base_out = generate_response(modelbase, s)\n",
    "    \n",
    "    print(f\"\\n Sentence: {s}\")\n",
    "    print(f\"Explain task Flan-T5: {A_out}\")\n",
    "    print(f\"Base Flan-T5: {base_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12a1e8-eee8-4bde-93e1-e0072c26c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = \"google/flan-t5-base\"\n",
    "modelB_name = \"./model_final_taskB\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelB_name)\n",
    "\n",
    "# Load base model and fully fine-tuned model separately\n",
    "modelbase = AutoModelForSeq2SeqLM.from_pretrained(base_name).to(device)\n",
    "modelB = AutoModelForSeq2SeqLM.from_pretrained(modelB_name).to(device)\n",
    "\n",
    "def generate_response(model, sentence):\n",
    "    prompt = (\n",
    "        \"In exactly 1-2 sentences, explain what the speaker actually means by removing the sarcasm \"\n",
    "        \"and stating their true intended message directly. \"\n",
    "        \"Focus on the genuine sentiment or opinion being expressed beneath the sarcastic language.\\n\\n\"\n",
    "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=True,\n",
    "            temperature=2.3,       \n",
    "            top_p=0.6,            \n",
    "            top_k=60,\n",
    "            num_beams=10,           \n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.4,  \n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    text = re.sub(r\"^(Explanation|Answer|Response)\\s*:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "sentences = [\n",
    "    \"Yeah, join the military and help the empire spread destruction across the world in the name of human rights, get PTSD, become homeless, sounds like a great way to improve one's situation.\",\n",
    "    \"Sorry, I'm too busy eating lobster in my yacht (paid for with self-published comics profits) to comment. <USER> <USER>\",\n",
    "    \"Oh perfect, the fire alarm goes off right when I start my presentation.\",\n",
    "    \"I just love when my boss schedules a meeting during lunch.\",\n",
    "    \"Oh great, the printer jammed right before the deadline.\",\n",
    "    \"Wonderful, traffic is even worse than yesterday!\",\n",
    "    \"Yeah, because everyone totally loves working overtime for free.\",\n",
    "    \"Crying before I go into work... This is going to be a great night. #Sarcasm #WishItWasTrue\",\n",
    "    \"Oh sure, because staying up till 3am totally helps with productivity üòí\",\n",
    "    \"hey <user> thanks for making it easy for me to take my music with me . # ihateyourupdates\",\n",
    "    \"It could confuse your muscles and make muscle grow in places where you didn't actually work out.\",\n",
    "    \"Yay, 2-hour traffic for a 10-minute errand. Exactly what I needed üôÉ\",\n",
    "    \"This guy gets a gold star for such excellent parking in the handicap lot!\",\n",
    "    \"How else will we feel superior if not by our amazing taste in phones?\",\n",
    "    \"Guess I‚Äôll just refresh the page for the 20th time. Maybe that‚Äôll fix it ü§°\",\n",
    "    \"My phone dying at 5% is the highlight of my day.\",\n",
    "    \"parrot's previous owner obviously watched a lot of the price is right\",\n",
    "    \"Oh totally, I love when people reply ‚Äòk‚Äô to my long texts.\",\n",
    "    \"Gotta save our children from the dangers of text on a screen in a rhythm game\",\n",
    "    \"Great, another inspirational quote on LinkedIn. Just what I needed.\",\n",
    "    \"even aside from the blatant misogyny, this is great because we have so much space in our prisons!\"\n",
    "]\n",
    "\n",
    "\n",
    "for s in sentences:\n",
    "    base_out = generate_response(modelbase, s)\n",
    "    B_out = generate_response(modelB, s)\n",
    "    \n",
    "    print(f\"\\n Sentence: {s}\")\n",
    "    print(f\"Explain Task Flan-T5: {B_out}\")\n",
    "    print(f\"Base Flan-T5: {base_out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
