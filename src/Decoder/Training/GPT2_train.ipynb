{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94823be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer as Trainer\n",
    "from transformers import Seq2SeqTrainingArguments as TrainingArguments\n",
    "import torch\n",
    "import evaluate\n",
    "import optuna\n",
    "from optuna.exceptions import TrialPruned\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/LCS2-IIITD/Multimodal-Sarcasm-Explanation-MuSE/main/Dataset/\"\n",
    "MuSE_train_df = pd.read_csv(base_url + \"train_df.tsv\", sep=\"\\t\", header=None,\n",
    "                       names=[\"PID\",\"Caption\",\"Explanation\"])\n",
    "MuSE_val_df   = pd.read_csv(base_url + \"val_df.tsv\",   sep=\"\\t\", header=None,\n",
    "                       names=[\"PID\",\"Caption\",\"Explanation\"])\n",
    "MuSE_test_df  = pd.read_csv(base_url + \"test_df.tsv\",  sep=\"\\t\", header=None,\n",
    "                       names=[\"PID\",\"Caption\",\"Explanation\"])\n",
    "\n",
    "print(MuSE_train_df.head())\n",
    "print(MuSE_val_df.head())\n",
    "print(MuSE_test_df.head())\n",
    "print(MuSE_train_df.shape)\n",
    "print(MuSE_val_df.shape)\n",
    "print(MuSE_test_df.shape)\n",
    "\n",
    "path = kagglehub.dataset_download(\"prayag007/sarcasm-explanation\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "kag_train_df = pd.read_csv(f\"{path}/train_df.tsv\", sep=\"\\t\", header=0)\n",
    "kag_val_df   = pd.read_csv(f\"{path}/val_df.tsv\",   sep=\"\\t\", header=0)\n",
    "\n",
    "print(kag_train_df.head())\n",
    "print(kag_val_df.head())\n",
    "print(kag_train_df.shape)\n",
    "print(kag_val_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1605aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MuSE_train_df = MuSE_train_df.drop(columns=[\"PID\"])\n",
    "MuSE_val_df   = MuSE_val_df.drop(columns=[\"PID\"])\n",
    "MuSE_test_df  = MuSE_test_df.drop(columns=[\"PID\"])\n",
    "\n",
    "for df in [kag_train_df, kag_val_df]:\n",
    "    for col in [\"Unnamed: 0\", \"pid\"]:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "print(kag_train_df.columns)\n",
    "\n",
    "MuSE_train_df.rename(columns={\"Caption\": \"text\", \"Explanation\": \"explanation\"}, inplace=True)\n",
    "MuSE_val_df.rename(columns={\"Caption\": \"text\", \"Explanation\": \"explanation\"}, inplace=True)\n",
    "MuSE_test_df.rename(columns={\"Caption\": \"text\", \"Explanation\": \"explanation\"}, inplace=True)\n",
    "\n",
    "kag_train_df.rename(columns={\"text\": \"text\", \"explanation\": \"explanation\"}, inplace=True)\n",
    "kag_val_df.rename(columns={\"text\": \"text\", \"explanation\": \"explanation\"}, inplace=True)\n",
    "\n",
    "MuSE_train_df[\"source\"] = \"muse\"\n",
    "MuSE_val_df[\"source\"] = \"muse\"\n",
    "MuSE_test_df[\"source\"] = \"muse\"\n",
    "kag_train_df[\"source\"] = \"kaggle\"\n",
    "kag_val_df[\"source\"] = \"kaggle\"\n",
    "\n",
    "combined_train = pd.concat([MuSE_train_df, kag_train_df], ignore_index=True)\n",
    "combined_val   = pd.concat([MuSE_val_df, kag_val_df], ignore_index=True)\n",
    "\n",
    "print(combined_train.shape)\n",
    "print(combined_val.shape)\n",
    "combined_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target(row):\n",
    "    exp = str(row[\"explanation\"]).strip()\n",
    "    return f\" {exp}\"\n",
    "\n",
    "combined_train[\"target_text\"] = combined_train.apply(build_target, axis=1)\n",
    "combined_val[\"target_text\"]   = combined_val.apply(build_target, axis=1)\n",
    "\n",
    "train_ds = Dataset.from_pandas(combined_train[[\"text\", \"target_text\"]])\n",
    "val_ds   = Dataset.from_pandas(combined_val[[\"text\", \"target_text\"]])\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "MAX_LEN = 160\n",
    "\n",
    "def preprocess(examples):\n",
    "    prompts = [\n",
    "        f\"Explain why the following sentence sounds sarcastic:\\n\"\n",
    "        f\"Sentence: {t}\\n\"\n",
    "        f\"Explanation:{exp}\"\n",
    "        for t, exp in zip(examples[\"text\"], examples[\"target_text\"])\n",
    "    ]\n",
    "    out = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()  \n",
    "    return out\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok   = val_ds.map(preprocess,   batched=True, remove_columns=val_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataset(ds, fraction=0.2, seed=4213):\n",
    "    total = len(ds)\n",
    "    subset_size = int(total * fraction)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(total, subset_size, replace=False)\n",
    "    return ds.select(indices.tolist())\n",
    "\n",
    "train_tok_sub = subset_dataset(train_tok, 0.2)\n",
    "val_tok_sub   = subset_dataset(val_tok, 0.2)\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    pred_texts  = tokenizer.batch_decode(preds,  skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    r = rouge.compute(predictions=pred_texts, references=label_texts)\n",
    "    return {\"rougeL\": round(r[\"rougeL\"], 4)}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return torch.argmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-5, 3e-5, 1e-4, 3e-4])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    dropout_rate = trial.suggest_categorical(\"dropout_rate\", [0.1, 0.2, 0.3])\n",
    "    weight_decay = trial.suggest_categorical(\"weight_decay\", [0.0, 0.01, 0.05])\n",
    "    warmup_ratio = trial.suggest_categorical(\"warmup_ratio\", [0.03, 0.06, 0.1])\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        model.config.attn_pdrop = dropout_rate\n",
    "        model.config.resid_pdrop = dropout_rate\n",
    "        model.config.embd_pdrop = dropout_rate\n",
    "\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.use_cache = False\n",
    "        model.to(\"cpu\")\n",
    "\n",
    "        collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=max(2, batch_size // 2),\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=weight_decay,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            predict_with_generate=True,\n",
    "            gradient_accumulation_steps=max(1, 32 // batch_size),\n",
    "            fp16=False,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_tok_sub,\n",
    "            eval_dataset=val_tok_sub,\n",
    "            data_collator=collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        rougeL = eval_results.get(\"eval_rougeL\", 0.0)\n",
    "\n",
    "        return rougeL\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"Trial {trial.number} failed due to OOM â€” skipping.\")\n",
    "            raise TrialPruned()\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"gpt2_full_finetune_optuna\")\n",
    "study.optimize(objective, n_trials=6)\n",
    "\n",
    "\n",
    "os.makedirs(\"./optuna_results\", exist_ok=True)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params\n",
    "best_params[\"best_rougeL\"] = best_trial.value\n",
    "\n",
    "json_path = \"./optuna_results/best_gpt2_params.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "print(f\"\\nSaved best GPT-2 parameters to {json_path}\")\n",
    "print(json.dumps(best_params, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff542c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = \"./optuna_results/best_gpt2_params.json\"\n",
    "if not os.path.exists(params_path):\n",
    "    raise FileNotFoundError(f\"Cannot find {params_path}. Run Optuna tuning first.\")\n",
    "\n",
    "with open(params_path, \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "learning_rate = best_params[\"learning_rate\"]\n",
    "batch_size    = best_params[\"batch_size\"]\n",
    "dropout_rate  = best_params[\"dropout_rate\"]\n",
    "weight_decay  = best_params[\"weight_decay\"]\n",
    "warmup_ratio  = best_params[\"warmup_ratio\"]\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "model.config.attn_pdrop  = dropout_rate\n",
    "model.config.resid_pdrop = dropout_rate\n",
    "model.config.embd_pdrop  = dropout_rate\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    pred_texts  = tokenizer.batch_decode(preds,  skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    r = rouge.compute(predictions=pred_texts, references=label_texts)\n",
    "    return {\"rougeL\": round(r[\"rougeL\"], 4)}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return torch.argmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "=\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=max(2, batch_size // 2),\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    gradient_accumulation_steps=max(1, 32 // batch_size),\n",
    "    fp16=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "trainer.train()\n",
    "\n",
    "save_dir = \"./gpt2_full_sarcasm_final\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "torch.save(model.state_dict(), f\"{save_dir}/gpt2_full_sarcasm_final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab29e5b",
   "metadata": {},
   "source": [
    "# Full fine-tuned generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1401ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"gpt2\"\n",
    "fine_tuned_path = \"./gpt2_full_sarcasm_final\"   \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name).to(device)\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(fine_tuned_path).to(device)\n",
    "\n",
    "def generate_response(model, sentence):\n",
    "    prompt = (\n",
    "        \"Explain why this text is sarcastic\"\n",
    "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
    "        \"Explanation:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        num_beams=1,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # keep only what comes after \"Only output the Explanation:\"\n",
    "    if \"Explanation:\" in text:\n",
    "        text = text.split(\"Explanation:\")[-1].strip()\n",
    "\n",
    "    # cut off anything that looks like the prompt itself\n",
    "    text = re.sub(\n",
    "        r\"^(Explain why.*?Explanation:\\s*)\", \"\", text, flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    # remove stray prefixes\n",
    "    text = re.sub(r\"^(Explanation|Answer|Response)\\s*[:\\-]\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"Oh perfect, the fire alarm goes off right when I start my presentation.\",\n",
    "    \"I just love when my boss schedules a meeting during lunch.\",\n",
    "    \"Oh great, the printer jammed right before the deadline.\",\n",
    "    \"Wonderful, traffic is even worse than yesterday!\",\n",
    "    \"Yeah, because everyone totally loves working overtime for free.\",\n",
    "    \"Crying before I go into work... This is going to be a great night. #Sarcasm #WishItWasTrue\",\n",
    "    \"Oh sure, because staying up till 3am totally helps with productivity ðŸ˜’\",\n",
    "    \"hey <user> thanks for making it easy for me to take my music with me . # ihateyourupdates\",\n",
    "    \"It could confuse your muscles and make muscle grow in places where you didn't actually work out.\",\n",
    "    \"Yay, 2-hour traffic for a 10-minute errand. Exactly what I needed ðŸ™ƒ\",\n",
    "    \"This guy gets a gold star for such excellent parking in the handicap lot!\",\n",
    "    \"How else will we feel superior if not by our amazing taste in phones?\",\n",
    "    \"Guess Iâ€™ll just refresh the page for the 20th time. Maybe thatâ€™ll fix it ðŸ¤¡\",\n",
    "    \"My phone dying at 5% is the highlight of my day.\",\n",
    "    \"parrot's previous owner obviously watched a lot of the price is right\",\n",
    "    \"Oh totally, I love when people reply â€˜kâ€™ to my long texts.\",\n",
    "    \"Gotta save our children from the dangers of text on a screen in a rhythm game\",\n",
    "    \"Great, another inspirational quote on LinkedIn. Just what I needed.\",\n",
    "    \"even aside from the blatant misogyny, this is great because we have so much space in our prisons!\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    base_out = generate_response(base_model, s)\n",
    "    fine_out = generate_response(fine_tuned_model, s)\n",
    "\n",
    "    print(f\"\\n Sentence: {s}\")\n",
    "    print(f\"Base GPT-2: {base_out}\")\n",
    "    print(f\"Fine-tuned GPT-2: {fine_out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
