{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94823be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import evaluate\n",
    "import optuna\n",
    "from optuna.exceptions import TrialPruned\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/LCS2-IIITD/Multimodal-Sarcasm-Explanation-MuSE/main/Dataset/\"\n",
    "MuSE_train_df = pd.read_csv(base_url + \"train_df.tsv\", sep=\"\\t\", header=None,\n",
    "                       names=[\"PID\",\"Caption\",\"Explanation\"])\n",
    "MuSE_val_df   = pd.read_csv(base_url + \"val_df.tsv\",   sep=\"\\t\", header=None,\n",
    "                       names=[\"PID\",\"Caption\",\"Explanation\"])\n",
    "MuSE_test_df  = pd.read_csv(base_url + \"test_df.tsv\",  sep=\"\\t\", header=None,\n",
    "                       names=[\"PID\",\"Caption\",\"Explanation\"])\n",
    "\n",
    "print(MuSE_train_df.head())\n",
    "print(MuSE_val_df.head())\n",
    "print(MuSE_test_df.head())\n",
    "print(MuSE_train_df.shape)\n",
    "print(MuSE_val_df.shape)\n",
    "print(MuSE_test_df.shape)\n",
    "\n",
    "path = kagglehub.dataset_download(\"prayag007/sarcasm-explanation\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "kag_train_df = pd.read_csv(f\"{path}/train_df.tsv\", sep=\"\\t\", header=0)\n",
    "kag_val_df   = pd.read_csv(f\"{path}/val_df.tsv\",   sep=\"\\t\", header=0)\n",
    "\n",
    "print(kag_train_df.head())\n",
    "print(kag_val_df.head())\n",
    "print(kag_train_df.shape)\n",
    "print(kag_val_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1605aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MuSE_train_df = MuSE_train_df.drop(columns=[\"PID\"])\n",
    "MuSE_val_df   = MuSE_val_df.drop(columns=[\"PID\"])\n",
    "MuSE_test_df  = MuSE_test_df.drop(columns=[\"PID\"])\n",
    "\n",
    "# Drop index-like columns only\n",
    "for df in [kag_train_df, kag_val_df]:\n",
    "    for col in [\"Unnamed: 0\", \"pid\"]:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Keep target_of_sarcasm\n",
    "print(kag_train_df.columns)\n",
    "\n",
    "MuSE_train_df.rename(columns={\"Caption\": \"text\", \"Explanation\": \"explanation\"}, inplace=True)\n",
    "MuSE_val_df.rename(columns={\"Caption\": \"text\", \"Explanation\": \"explanation\"}, inplace=True)\n",
    "MuSE_test_df.rename(columns={\"Caption\": \"text\", \"Explanation\": \"explanation\"}, inplace=True)\n",
    "\n",
    "kag_train_df.rename(columns={\"text\": \"text\", \"explanation\": \"explanation\"}, inplace=True)\n",
    "kag_val_df.rename(columns={\"text\": \"text\", \"explanation\": \"explanation\"}, inplace=True)\n",
    "\n",
    "MuSE_train_df[\"source\"] = \"muse\"\n",
    "MuSE_val_df[\"source\"] = \"muse\"\n",
    "MuSE_test_df[\"source\"] = \"muse\"\n",
    "kag_train_df[\"source\"] = \"kaggle\"\n",
    "kag_val_df[\"source\"] = \"kaggle\"\n",
    "\n",
    "combined_train = pd.concat([MuSE_train_df, kag_train_df], ignore_index=True)\n",
    "combined_val   = pd.concat([MuSE_val_df, kag_val_df], ignore_index=True)\n",
    "\n",
    "print(combined_train.shape)\n",
    "print(combined_val.shape)\n",
    "combined_train.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target(row):\n",
    "    \"\"\"Constructs the output text (label) for training.\"\"\"\n",
    "    exp = str(row[\"explanation\"]).strip()\n",
    "    return f\"Explanation: {exp}\"\n",
    "\n",
    "# Apply to both train/validation sets\n",
    "combined_train[\"target_text\"] = combined_train.apply(build_target, axis=1)\n",
    "combined_val[\"target_text\"]   = combined_val.apply(build_target, axis=1)\n",
    "\n",
    "print(combined_train.head())\n",
    "\n",
    "# Keep only required columns\n",
    "train_ds = Dataset.from_pandas(combined_train[[\"text\", \"target_text\"]])\n",
    "val_ds   = Dataset.from_pandas(combined_val[[\"text\", \"target_text\"]])\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "MAX_SRC_LEN = 128\n",
    "MAX_TGT_LEN = 64\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [\n",
    "        (\n",
    "            \"Task: Explain why the following sentence is sarcastic.\\n\"\n",
    "            \"Input Sentence: \" + text + \"\\n\"\n",
    "            \"Output Format: Explanation: <reason>.\"\n",
    "        )\n",
    "        for text in examples[\"text\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SRC_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples[\"target_text\"],\n",
    "        max_length=MAX_TGT_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok   = val_ds.map(preprocess,   batched=True, remove_columns=val_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataset(ds, fraction=0.2, seed=4213):\n",
    "    total = len(ds)\n",
    "    subset_size = int(total * fraction)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(total, subset_size, replace=False)\n",
    "    return ds.select(indices.tolist())\n",
    "\n",
    "train_tok_sub = subset_dataset(train_tok, 0.2)\n",
    "val_tok_sub   = subset_dataset(val_tok, 0.2)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    rouge_result = rouge.compute(predictions=pred_texts, references=label_texts)\n",
    "    return {\"rougeL\": round(rouge_result[\"rougeL\"], 4)}\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-5, 3e-5, 1e-4, 3e-4])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    dropout_rate = trial.suggest_categorical(\"dropout_rate\", [0.1, 0.2, 0.3])\n",
    "    weight_decay = trial.suggest_categorical(\"weight_decay\", [0.0, 0.01, 0.05])\n",
    "    warmup_ratio = trial.suggest_categorical(\"warmup_ratio\", [0.03, 0.06, 0.1])\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        model.config.dropout_rate = dropout_rate\n",
    "        model.config.attention_dropout_rate = dropout_rate\n",
    "        model.to(\"cpu\")\n",
    "\n",
    "        collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=max(2, batch_size // 2), #To prevent OOM\n",
    "            num_train_epochs=1,              \n",
    "            weight_decay=weight_decay,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            predict_with_generate=True,\n",
    "            gradient_accumulation_steps=max(2, batch_size // 2), #To prevent OOM\n",
    "            fp16=False,                      \n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_tok_sub,\n",
    "            eval_dataset=val_tok_sub,\n",
    "            data_collator=collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        rougeL = eval_results.get(\"eval_rougeL\", 0.0)\n",
    "\n",
    "        return rougeL\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"Trial {trial.number} failed due to OOM ‚Äî skipping.\")\n",
    "            raise TrialPruned()\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"t5_full_finetune_optuna\")\n",
    "study.optimize(objective, n_trials=6)\n",
    "\n",
    "\n",
    "save_dir = \"./optuna_results\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params\n",
    "best_params[\"best_rougeL\"] = best_trial.value\n",
    "\n",
    "json_path = os.path.join(save_dir, \"best_t5_params.json\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "print(f\"\\nSaved best parameters to {json_path}\")\n",
    "print(json.dumps(best_params, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "\n",
    "params_path = \"./optuna_results/best_t5_params.json\"\n",
    "if not os.path.exists(params_path):\n",
    "    raise FileNotFoundError(f\"Cannot find {params_path}. Run Optuna tuning first.\")\n",
    "\n",
    "with open(params_path, \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "print(\"Loaded best hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "learning_rate = best_params[\"learning_rate\"]\n",
    "batch_size    = best_params[\"batch_size\"]\n",
    "dropout_rate  = best_params[\"dropout_rate\"]\n",
    "weight_decay  = best_params[\"weight_decay\"]\n",
    "warmup_ratio  = best_params[\"warmup_ratio\"]\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "model.config.dropout_rate = dropout_rate\n",
    "model.config.attention_dropout_rate = dropout_rate\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    rouge_result = rouge.compute(predictions=pred_texts, references=label_texts)\n",
    "    return {\"rougeL\": round(rouge_result[\"rougeL\"], 4)}\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=max(2, batch_size // 2), #To prevent OOM\n",
    "    num_train_epochs=5,                \n",
    "    weight_decay=weight_decay,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,           \n",
    "    generation_num_beams=1,\n",
    "    logging_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=max(2, batch_size // 2), #To prevent OOM\n",
    "    fp16=False,                         \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "save_dir = \"./flan_t5_full_sarcasm_final\"\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "torch.save(model.state_dict(), f\"{save_dir}/flan_t5_full_sarcasm_final.pt\")\n",
    "\n",
    "print(\"\\nFull fine-tuning complete and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab29e5b",
   "metadata": {},
   "source": [
    "# Full fine-tuned generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1401ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"google/flan-t5-base\"\n",
    "fine_tuned_path = \"./flan_t5_full_sarcasm_final\"  # full fine-tuned model path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_path)\n",
    "\n",
    "# Load base model and fully fine-tuned model separately\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name).to(device)\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(fine_tuned_path).to(device)\n",
    "\n",
    "def generate_response(model, sentence):\n",
    "    prompt = (\n",
    "        \"Explain why this text is sarcastic\"\n",
    "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        num_beams=1,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        length_penalty=1.2,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    text = re.sub(r\"^(Explanation|Answer|Response)\\s*:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3Ô∏è‚É£ Compare models\n",
    "# -------------------------------------------------\n",
    "sentences = [\n",
    "    \"Oh perfect, the fire alarm goes off right when I start my presentation.\",\n",
    "    \"I just love when my boss schedules a meeting during lunch.\",\n",
    "    \"Oh great, the printer jammed right before the deadline.\",\n",
    "    \"Wonderful, traffic is even worse than yesterday!\",\n",
    "    \"Yeah, because everyone totally loves working overtime for free.\",\n",
    "    \"Crying before I go into work... This is going to be a great night. #Sarcasm #WishItWasTrue\",\n",
    "    \"Oh sure, because staying up till 3am totally helps with productivity üòí\",\n",
    "    \"hey <user> thanks for making it easy for me to take my music with me . # ihateyourupdates\",\n",
    "    \"It could confuse your muscles and make muscle grow in places where you didn't actually work out.\",\n",
    "    \"Yay, 2-hour traffic for a 10-minute errand. Exactly what I needed üôÉ\",\n",
    "    \"This guy gets a gold star for such excellent parking in the handicap lot!\",\n",
    "    \"How else will we feel superior if not by our amazing taste in phones?\",\n",
    "    \"Guess I‚Äôll just refresh the page for the 20th time. Maybe that‚Äôll fix it ü§°\",\n",
    "    \"My phone dying at 5% is the highlight of my day.\",\n",
    "    \"parrot's previous owner obviously watched a lot of the price is right\",\n",
    "    \"Oh totally, I love when people reply ‚Äòk‚Äô to my long texts.\",\n",
    "    \"Gotta save our children from the dangers of text on a screen in a rhythm game\",\n",
    "    \"Great, another inspirational quote on LinkedIn. Just what I needed.\",\n",
    "    \"even aside from the blatant misogyny, this is great because we have so much space in our prisons!\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    base_out = generate_response(base_model, s)\n",
    "    fine_out = generate_response(fine_tuned_model, s)\n",
    "\n",
    "    print(f\"\\nSentence: {s}\")\n",
    "    print(f\"Base Flan-T5: {base_out}\")\n",
    "    print(f\"Fine-tuned Flan-T5: {fine_out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
