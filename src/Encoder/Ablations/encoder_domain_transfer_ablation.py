# -*- coding: utf-8 -*-
"""data_prep (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fMnERHJ5AFLGlN7Qi6DhIZTOUEkRqTGS
"""

!pip install optuna
import kagglehub
import numpy as np
import re
import pandas as pd
from tqdm import tqdm
tqdm.pandas()
from sklearn.model_selection import train_test_split
from datasets import Dataset
import torch, random, numpy as np
import os, time
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
import matplotlib.pyplot as plt
import optuna

# Load data from kaggle
headlinespath = kagglehub.dataset_download("rmisra/news-headlines-dataset-for-sarcasm-detection")
twitterpath   = kagglehub.dataset_download("nikhiljohnk/tweets-with-sarcasm-and-irony")
redditpath    = kagglehub.dataset_download("sherinclaudia/sarcastic-comments-on-reddit")

# Headline datasets
headlines1 = pd.read_json(f"{headlinespath}/Sarcasm_Headlines_Dataset.json", lines=True)
headlines2 = pd.read_json(f"{headlinespath}/Sarcasm_Headlines_Dataset_v2.json", lines=True)
data_headlines = pd.concat([headlines1, headlines2], ignore_index=True)

label_headlines = (
    data_headlines
    .drop(columns=["article_link"], errors="ignore")
    .rename(columns={"headline": "text", "is_sarcastic": "is_sarcastic"})
    .assign(source="headline")
)

# Reddit dataset
reddit = pd.read_csv(f"{redditpath}/train-balanced-sarcasm.csv")

#  Reddit: stratified fixed-size sampling (N = 222,664, same as other models)
N = 222_664
counts = reddit['label'].value_counts()
props  = counts / counts.sum()
take   = (props * N).round().astype(int)

# cap by available rows per label
take = take.clip(upper=counts)

# fix total to hit N after rounding/clipping
diff = N - int(take.sum())

if diff > 0:
    # add where there's headroom, starting from largest props
    order = props.sort_values(ascending=False).index.tolist()
    i = 0
    while diff > 0 and any((counts - take) > 0):
        lbl = order[i % len(order)]
        if take[lbl] < counts[lbl]:
            take[lbl] += 1
            diff -= 1
        i += 1
elif diff < 0:
    # remove starting from smallest props
    order = props.sort_values(ascending=True).index.tolist()
    i = 0
    while diff < 0 and any(take > 0):
        lbl = order[i % len(order)]
        if take[lbl] > 0:
            take[lbl] -= 1
            diff += 1
        i += 1

# sample per label
reddit_sample = (
    reddit.groupby("label", group_keys=False)
          .apply(lambda x: x.sample(n=int(take.loc[x.name]), random_state=42))
          .reset_index(drop=True)
)

label_reddit = (
    reddit_sample
    .rename(columns={"comment": "text", "label": "is_sarcastic"})
    .assign(source="reddit")
    [["text", "is_sarcastic", "source"]]
)

# Twitter dataset (train + test)
train_twitter = pd.read_csv(f"{twitterpath}/train.csv")
test_twitter  = pd.read_csv(f"{twitterpath}/test.csv")

# Filter out figurative & assign labels
def preprocess_twitter(df):
    df = df[df["class"] != "figurative"].copy()
    df["is_sarcastic"] = np.where(df["class"] == "regular", 0, 1)
    df = df.rename(columns={"tweets": "text"}).drop(columns=["class"])
    df["source"] = "twitter"
    return df[["text", "is_sarcastic", "source"]]

label_twitter = pd.concat([preprocess_twitter(train_twitter),
                           preprocess_twitter(test_twitter)],
                          ignore_index=True)

# - SPLITS
# Train/Val pool = Reddit only
trainval_df = label_reddit.copy()

# Test set = everything non-Reddit
test_df = pd.concat([label_headlines, label_twitter], ignore_index=True)

print("Train/Val (Reddit) size:", len(trainval_df))   # should be ~222,664
print("Test (Non-Reddit) size:", len(test_df))
print("Test sources:\n", test_df["source"].value_counts())
print("Train/Val label balance:\n", trainval_df["is_sarcastic"].value_counts())

# create a stratified train/val split from the Reddit pool
from sklearn.model_selection import train_test_split
train_df, val_df = train_test_split(
    trainval_df, test_size=0.1, random_state=42, stratify=trainval_df["is_sarcastic"]
)
print("Train size:", len(train_df), "Val size:", len(val_df))

def clean_text(txt):
    if pd.isna(txt):
        return np.nan

    txt = str(txt)
    # Replace URLs
    txt = re.sub(r"http\S+|www\S+|https\S+", "<URL>", txt)
    # Replace user mentions
    txt = re.sub(r"@\w+", "<USER>", txt)
    # Remove explicit sarcasm/irony tags (case-insensitive)
    txt = re.sub(r"#\s*(sarcasm|irony)\b", "", txt, flags=re.IGNORECASE)
    # Remove '#' from hashtags (keep the word)
    txt = re.sub(r"#(\w+)", r"\1", txt)
    # Normalize whitespace
    txt = re.sub(r"\s+", " ", txt).strip()
    return txt

trainval_df["text"] = trainval_df["text"].progress_apply(clean_text)
test_df["text"] = test_df["text"].progress_apply(clean_text)

train_df = train_df.rename(columns={"is_sarcastic": "label"})
val_df   = val_df.rename(columns={"is_sarcastic": "label"})
test_df  = test_df.rename(columns={"is_sarcastic": "label"})

SEED = 4213
np.random.seed(SEED)
torch.manual_seed(SEED)

# make sure there are no NaN texts, but keep all rows
for _df in (train_df, val_df, test_df):
    _df["text"] = _df["text"].fillna("<EMPTY>").astype(str)
    _df["label"] = _df["label"].astype(int)

# Tokenization
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

train_ds = Dataset.from_pandas(train_df.reset_index(drop=True)).map(tokenize_function, batched=True)
val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True)).map(tokenize_function, batched=True)
test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True)).map(tokenize_function, batched=True)

cols = ["input_ids", "attention_mask", "label"]
train_ds.set_format(type="torch", columns=cols)
val_ds.set_format(type="torch", columns=cols)
test_ds.set_format(type="torch", columns=cols)

# Metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds)
    return {"accuracy": acc, "f1": f1}

# Optuna: tune on Reddit train/val
def objective(trial):
    model = AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased", num_labels=2
    )

    lr = trial.suggest_float("lr", 1e-6, 5e-5, log=True)
    batch_size = trial.suggest_categorical("batch_size", [8, 16, 32])
    weight_decay = trial.suggest_float("weight_decay", 0.0, 0.1)
    num_epochs = trial.suggest_int("epochs", 2, 4)

    args = TrainingArguments(
        output_dir=f"results/trial_{trial.number}",
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_epochs,
        weight_decay=weight_decay,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        logging_dir="logs",
        logging_steps=20,
        seed=SEED,
        disable_tqdm=False,
        report_to = 'none'
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        compute_metrics=compute_metrics,
        tokenizer=tokenizer,
    )

    trainer.train()
    m = trainer.evaluate(eval_dataset=val_ds)
    return m["eval_f1"]

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=6)

print("Best trial:")
print(f"  Value (F1): {study.best_trial.value}")
for k, v in study.best_trial.params.items():
    print(f"  {k}: {v}")

# Final training with best params
best_params = study.best_trial.params
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=2
)

training_args = TrainingArguments(
    output_dir="full_finetuned_distilbert",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=best_params.get("lr", 3e-5),
    per_device_train_batch_size=best_params.get("batch_size", 16),
    per_device_eval_batch_size=best_params.get("batch_size", 16),
    num_train_epochs=best_params.get("epochs", 3),
    weight_decay=best_params.get("weight_decay", 0.01),
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_dir="logs_final",
    seed=SEED,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)

print("Starting final full fine-tuning (Reddit train/val)...")
t0 = time.time()
trainer.train()
train_time = time.time() - t0
print(f"Training completed in {train_time/60:.2f} minutes")

# Evaluate on Reddit val + Non-Reddit test
val_metrics = trainer.evaluate(eval_dataset=val_ds)

test_preds = trainer.predict(test_ds)
logits = test_preds.predictions
labels = test_preds.label_ids

def test_metrics_fn(logits, labels):
    preds = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds)
    try:
        probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()
        roc = roc_auc_score(labels, probs)
    except:
        roc = float("nan")
    return {"accuracy": acc, "f1": f1, "roc_auc": roc}

test_metrics = test_metrics_fn(logits, labels)

print("\n===== VALIDATION (Reddit) METRICS =====")
for k, v in val_metrics.items():
    if k.startswith("eval_"):
        print(f"{k}: {v:.4f}")

print("\n===== TEST (Non-Reddit) METRICS =====")
for k, v in test_metrics.items():
    print(f"{k}: {v:.4f}")

# Artifacts
os.makedirs("results", exist_ok=True)

# Confusion Matrix (Non-Reddit test)
y_true = labels
y_pred = np.argmax(logits, axis=-1)
cm = confusion_matrix(y_true, y_pred)

fig, ax = plt.subplots(figsize=(6, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Not Sarcastic", "Sarcastic"])
disp.plot(cmap="Blues", ax=ax, colorbar=False)
plt.title("Confusion Matrix - Test Set (Non-Reddit)")
plt.tight_layout()
plt.savefig("results/confusion_matrix_test.png", bbox_inches="tight", dpi=300)
plt.close(fig)
print("Confusion matrix saved to results/confusion_matrix_test.png")

# ROC Curve (Non-Reddit test)
probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()
fpr, tpr, thr = roc_curve(y_true, probs)
roc_auc = auc(fpr, tpr)

fig, ax = plt.subplots(figsize=(6, 6))
ax.plot(fpr, tpr, lw=2, label=f"AUC = {roc_auc:.3f}")
ax.plot([0, 1], [0, 1], linestyle="--")
ax.set_xlabel("False Positive Rate")
ax.set_ylabel("True Positive Rate")
ax.set_title("ROC Curve - Test Set (Non-Reddit)")
ax.legend(loc="lower right")
plt.tight_layout()
plt.savefig("results/roc_curve_test.png", bbox_inches="tight", dpi=300)
plt.close(fig)
print("ROC curve saved to results/roc_curve_test.png")

# Save model
model_dir = "results/best_full_finetuned_distilbert"
trainer.save_model(model_dir)
pt_path = os.path.join("results", "best_full_finetuned_distilbert.pt")
torch.save(model.state_dict(), pt_path)
print(f"ðŸ’¾ Model weights saved as {pt_path}")

# Summary
summary = {
    "Best Parameters": study.best_trial.params,
    "Training Time (min)": round(train_time / 60, 2),
    "Validation Metrics": {k: round(v, 4) for k, v in val_metrics.items() if k.startswith("eval_")},
    "Test Metrics": {k: round(v, 4) for k, v in test_metrics.items()},
    "ROC AUC (Test)": round(roc_auc, 4)
}
print("\n===== SUMMARY =====")
for k, v in summary.items():
    print(f"{k}: {v}")
